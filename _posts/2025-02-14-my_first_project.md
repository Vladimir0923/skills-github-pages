---
title: "PROFESSIONAL CERTIFICATE. Google Data Analytics"
date: 2025-02-14
---

## **PROFESSIONAL CERTIFICATE. Google Data Analytics**

## Case Study 2

## **Author**: Vladimir Ruiz Alvarez

## Background

**Bellabeat**, a manufacturer of high-tech products focused on women's health, is a small but successful company that has the potential to establish a significant presence in the global market for smart devices. The Creative Director of Bellabeat believes that analyzing the activity data from smart devices could unveil new business opportunities for the company. Given this scenario, she proposes to the company's marketing analytics team to **analyze the data from the smart devices to understand how consumers use their devices**. The findings could assist in shaping the company's marketing strategy.

## Data analysis process

# I) **Ask**

Having a clear understanding of the background is the important first step in this stage. The background sets the context or theoretical framework that will enable understanding and clarify the path to be followed in conducting the subsequent phases of the analysis process. In this stage, the problem that needs to be addressed, the requirements and expectations of stakeholders, the objectives that will provide answers to the problem, as well as the means and resources necessary to achieve the objectives are clarified.

**Bellabeat Products**

* **Bellabeat App**: Provides data on physical activity, sleep, stress, menstrual cycle, and mindfulness habits. This app connects to Bellabeat's line of smart wellness products.
* **Leaf**: A device that can be worn as a bracelet, necklace, or clip. It tracks physical activity, sleep, and stress.
* **Time**: A watch that also tracks the user's physical activity, sleep, and stress.
* **Spring**: A smart water bottle for daily tracking of water consumption and user hydration levels.
* **Bellabeat Membership**: A subscription-based membership program that provides 24/7 access to personalized guidance on nutrition, physical activity, sleep, health and beauty, and mindfulness based on the user's lifestyle and goals.

# 1.1) Questions for the analysis of the problem posed by Bellabeat 

1.	#### What are some usage trends of smart devices?
2.	#### How could these trends be applied to Bellabeat's customers?
3.	#### How could these trends help influence Bellabeat's marketing strategy?

# 1.2) Business task

#### Identify potential opportunities for growth and provide suggestions for improving the Bellabeat marketing strategy based on trends in smart device usage.

# II) **Prepare**

# 2.1) About the data

* **Source, nature, and storage of the data to be used**: The data used in this analysis originated from Fitbit's physical activity tracking records, which were obtained from a publicly available dataset on [Kaggle](https://www.kaggle.com/arashnic) via [Mobius](https://www.kaggle.com/arashnic). These datasets were generated by respondents to a distributed survey via Amazon Mechanical Turk between 03/12/2016-05/12/2016. A group of eligible Fitbit users consented to the submission of personal tracker data, including physical activity, heart rate, and sleep monitoring. Variation between output represents use of different types of Fitbit trackers and individual tracking behaviors / preferences. The dataset was downloaded to the computer from where the entire analysis was conducted. **License**: [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/). **Source**: Furberg, R., Brinton, J., Keating, M., & Ortiz, A. (2016). Crowd-sourced Fitbit datasets 03.12.2016-05.12.2016 [[Data set]. Zenodo. https://doi.org/10.5281/zenodo.53894

* **Data organization**: The data is organized into 18 databases, each with different structures and types of data. All databases have a common "**identification**" column (**Id**), which allows establishing relationships between them. Additionally, all databases have a column with date or date/time data. Fourteen out of the 18 databases contain information on physical activity, including calorie expenditure, intensity levels, and step counts. One database includes heart rate data, two have sleep-related information, and the final one contains anthropometric data of the participating individuals (weight in pounds and kilograms, body fat, and body mass index).

* **Data bias or credibility issues (ROCCC)**:
  * **Reliable?**: Once the potential biases, errors, and limitations of the data are recognized (See more details in **Data Issues** below), it can be assumed that the data is reliable to the extent that it can be used for the intended purposes and interests, and yield trustworthy results that can guide the actions expected to be derived from these findings.
  * **Original?**: Yes, the data is original and citable.
  * **Comprehensive?**: Yes, it is comprehensive. 
  * **Current?**: No, it is not current; however, considering the study objectives, this "lack of currency" does not invalidate its use. 
  * **Cited?**: Yes, they are properly cited.
  
  
* **Authorization, privacy, security, and accessibility of the data**: The data collection method ensures the privacy and anonymity of the participating individuals. Authorization for data usage has been obtained, first from the individuals whose information was recorded by the smart devices (as reported by the source), and secondly, since the data is publicly available and accessible on the internet, it can be accessed by anyone who wishes to do so.

* **How it helps answer the main question**: The data was collected over a specific, well-defined period, enabling assessments of usage trends over time, which is the primary objective of the study.

# 2.2) Data Issues

  * There are two fundamental types of biases in these data: **selection bias** and **information bias**. **Selection bias** exists because the chosen dataset was not selected based on pre-established criteria to ensure representativeness of the population from which it originates. Therefore, any extrapolation of the results must be approached with extreme caution. **Information bias** arises due to a lack of control over the quality of the primary data collected. In other words, there were no predefined criteria to guarantee the quality of the information to be collected, such as the type of data to be collected, how it should be collected, who should collect it, in what format it should be stored, among other aspects. In this case, the available data consisted of information recorded by different types of smart devices, at any given time according to user habits and preferences, with a potential risk of missing or wrong data due to technical issues with the equipment (recording failures, battery failures, etc.).
  * It would have been interesting to have demographic data about the individuals.
  * It would have been beneficial to have documentation about the information collected in the different databases and each of their variables, including the units of measurement if possible. While it is possible to infer the recorded data for most variables, there are some variables where this is not the case, leaving their usage subject to the analyst's subjectivity.
  * It would have been helpful to have information about which type of device recorded each data point, as this detail would have facilitated more precise recommendations regarding the use of individual devices.

# 2.3) Conclusions about the data
The available data has limitations and deficiencies that have been previously analyzed. With these deficiencies, it is impossible to make inferences that can be extrapolated to other contexts in the traditional sense of statistical inference. However, once the data is cleaned and processed, it can be effectively used for the purposes proposed in this study: making recommendations to Bellabeat based on the observed trends in smart device usage. This information can be extracted successfully from this data.

# III) **Process**

# 3.1) Loading packages and datasets

### 3.1.1) Loading packages

```{r}
# Loading general packages
library(tidyverse) # Offers a comprehensive set of tools for working with data
library(lubridate) # Functions for working with date-times and time-spans
library(tidyr) # Facilitates data organization and cleaning for improved data handling
library(dplyr) # Simplifies data frame operations with efficient filtering, selecting, sorting, and summarizing
library(conflicted) # Resolves conflicts between packages to ensure smooth package interactions
conflicted::conflicts_prefer(dplyr::filter) # Specifies winners for conflicts when resolving conflicting functions
library(ggplot2) # Generates visually appealing and customizable data visualizations
library(gridExtra) # Offers functions for arranging multiple grid-based plots and creating tables
library(cowplot) # Provides tools for combining and arranging plots
library(patchwork) # Allows for advanced plot composition
```
### 3.1.2) Loading dataset

```{r}
# Loading databases
bd1 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv")
bd2 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/dailyCalories_merged.csv")
bd3 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/dailyIntensities_merged.csv")
bd4 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/dailySteps_merged.csv")
bd5 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/heartrate_seconds_merged.csv")
bd6 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlyCalories_merged.csv")
bd7 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlyIntensities_merged.csv")
bd8 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/hourlySteps_merged.csv")
bd9 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteCaloriesNarrow_merged.csv")
bd10 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteCaloriesWide_merged.csv")
bd11 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteIntensitiesNarrow_merged.csv")
bd12 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteIntensitiesWide_merged.csv")
bd13 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteMETsNarrow_merged.csv")
bd14 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteSleep_merged.csv")
bd15 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteStepsNarrow_merged.csv")
bd16 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/minuteStepsWide_merged.csv")
bd17 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/sleepDay_merged.csv")
bd18 <- read.csv("/kaggle/input/fitbit/mturkfitbit_export_4.12.16-5.12.16/Fitabase Data 4.12.16-5.12.16/weightLogInfo_merged.csv")
```

**Comments**: When loading the dataset, an attempt was made to use names as simple as possible in order to facilitate the handling of the data frames while writing the codes. **After making the decision regarding which databases to use for the analyses, the names were changed to ones that are easier to remember**. 

# 3.2) Exploring and cleaning the data

A preliminary review of the databases was conducted using **Microsoft Excel**. The **CSV** files were imported into **Excel** using the '**Import**' option, and the remaining work was done in **R**. In **R**, the files were loaded using the '**read.csv()**' function. The following functions were used to understand the structure of the databases: '**summary()**', '**View()**', '**glimpse()**', '**head()**', '**str()**', '**nrow()**', '**colnames()**', and '**n_distinct(data_base_name$variable_name)**'. These functions provided information such as the number and names of variables (columns), the number of rows, and the data types of each variable. The '**glimpse()**' and '**n_distinct()**' functions are part of the **dplyr package**, while the remaining functions used in this step are part of the R base package.

By using the following lines of code, the different Ids contained in each database can be reviewed:

```r
Ids <- select(data_base_name, Id) # selects the Id column from a specific database
print(unique(Ids)) # displays the Ids of a particular database 
```
The function '**select()**' is part of the **dplyr package**, which is included in the **tidyverse** collection. This function is used to select specific columns from a data frame. On the other hand, the '**unique()**' function is a base R function used to find unique values in a vector, matrix, or data frame, and when combined with '**print()**', it displays that information in the console.

The table below presents a summary of the overall information extracted from the databases.

<img width="1776" alt="fitbit_table_1" src="https://github.com/user-attachments/assets/63f78833-10aa-4397-8777-fda1c9f687f0" />

The **dailyActivity**, **dailyCalories**, **dailylntensities** and **dailySteps** databases had the same number of rows (**940**) and the same number of Ids (**33**). To check whether the last three databases were contained within the first one, the following code was used to compare **dailyActivity** with the remaining ones. Here is an example of the code to compare **dailyCalories** with **dailyActivity**:

```r
# Function: Compare columns
# Description: Compare specified columns between two databases

compare_columns <- function(database1, database2, column_name) {
  # Sort both databases by the specified column
  db1_sorted <- database1[order(database1[[column_name]]), ]
  db2_sorted <- database2[order(database2[[column_name]]), ]
  
  # Extract the columns to compare
  column_db1 <- db1_sorted[[column_name]]
  column_db2 <- db2_sorted[[column_name]]
  
  # Compare the two columns
  identical(column_db1, column_db2)
}

# Example usage to compare the "Id" columns of two databases
compare_columns(bd1, bd2, "Id")

# Example usage to compare the "Calories" columns of two databases
compare_columns(bd1, bd2, "Calories")
```
To ensure that no difference between two databases regarding the same variable or column is present, the function '**identical()**' should return **TRUE** with the result displayed in the console. In **all the comparisons** that were made, the **result** was **TRUE**, confirming that the variables had the same information. Therefore, it was concluded that there was no need to use **dailyCalories**, **dailylntensities**, and **dailySteps** since they were already included within **dailyActivity**.

For this task (sorting the data frames and extracting the desired columns, as well as comparing the two columns), there is no need to install or load any specific R package. The '**order()**' function, which sorts a vector or matrix based on the values in one or multiple columns, and the '**identical()**' function, which compares whether two objects are identical, in this case, if the two columns extracted from the data frames are equal, are included in the base R. Therefore, there is no need to load any additional package to use them.

It was decided to work with the databases **bd1** (**dailyActivity**), **bd6** (**hourlyCalories**), **bd17** (**sleepDay**) and **bd18** (**weightLoglnfo**).

>-   **bd1 (dailyActivity) + bd17 (sleepDay)**: These two databases were combined to investigate potential differences in device usage between weekends and weekdays, using calorie expenditure as a reference. The majority of the analyses were conducted using these two merged databases.
>
>-   **bd6 (hourlyCalories)**: Employed to analyze usage patterns throughout the day, also based on calorie expenditure. 
>
>-   **bd18 (weightLoglnfo)**: This database was utilized to describe the anthropometric variables whenever possible.
